import os
import json
import pandas as pd
import requests
from sentence_transformers import SentenceTransformer
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from langchain.schema import Document
import time  # Pour g√©rer les retries

# üìå Configuration
URL = "https://datahub.bordeaux-metropole.fr/api/explore/v2.1/catalog/datasets/met_agenda/records"
DATA_FILE = "reponse.json"
RESPONSE_CLEAN_FILE = "reponses_nettoye.json"
FAISS_INDEX_PATH = "faiss_index"

def fetch_data():
    """
    R√©cup√®re les donn√©es de Bordeaux M√©tropole et les sauvegarde dans DATA_FILE (JSON).
    G√®re les erreurs r√©seau et les retries en cas d'√©chec.
    
    Dans cette version, nous r√©cup√©rons jusqu'√† 25000 enregistrements (ce qui couvre plus de 24219 r√©sultats).
    """
    all_data = []
    offset = 0
    limit = 100         # Nombre de r√©sultats par requ√™te (limite impos√©e par l'API)
    max_records = 10000 # Nombre total de r√©sultats √† r√©cup√©rer
    max_retries = 5     # Nombre maximum de tentatives en cas d'√©chec

    while len(all_data) < max_records:
        params = {'limit': limit, 'offset': offset}
        retries = 0
        while retries < max_retries:
            try:
                response = requests.get(URL, params=params, timeout=10)
                if response.status_code == 200:
                    data = response.json().get('results', [])
                    if not data:
                        print("‚úÖ Fin des donn√©es (aucune nouvelle entr√©e trouv√©e).")
                        break
                    all_data.extend(data)
                    offset += limit
                    break  # Succ√®s : sortir de la boucle de retry
                else:
                    print(f"‚ö†Ô∏è Erreur HTTP {response.status_code}, tentative {retries + 1}/{max_retries}")
                    retries += 1
                    time.sleep(2)
            except requests.exceptions.RequestException as e:
                print(f"‚ö†Ô∏è Erreur r√©seau : {e}, tentative {retries + 1}/{max_retries}")
                retries += 1
                time.sleep(2)
        if retries == max_retries:
            print(f"‚ùå √âchec apr√®s {max_retries} tentatives. Arr√™t de la r√©cup√©ration.")
            break

    with open(DATA_FILE, "w", encoding="utf-8") as file:
        json.dump(all_data, file, indent=4, ensure_ascii=False)
    print(f"‚úÖ {len(all_data)} √©v√©nements enregistr√©s dans {DATA_FILE}")

def clean_and_filter_data():
    """
    Charge et filtre les donn√©es depuis DATA_FILE, en supprimant les colonnes inutiles et 
    en ne conservant que les √©v√©nements post√©rieurs au 1er janvier 2025.
    
    Le DataFrame est renomm√© pour retirer le pr√©fixe 'fields.' (le cas √©ch√©ant) et le r√©sultat est sauvegard√© 
    dans RESPONSE_CLEAN_FILE.
    """
    with open(DATA_FILE, "r", encoding="utf-8") as file:
        data = json.load(file)

    df = pd.json_normalize(data)
    # Retirer le pr√©fixe "fields." si pr√©sent
    df.columns = df.columns.str.replace(r'^fields\.', '', regex=True)
    
    # Liste des colonnes √† conserver
    desired_columns = [
        "uid", "title_fr", "description_fr", "conditions_fr", "location_city",
        "keywords_fr", "daterange_fr", "firstdate_begin", "accessibility_label_fr",
        "location_uid", "location_name", "location_address", "location_district",
        "location_postalcode", "location_department", "location_region", "location_countrycode",
        "location_image", "location_imagecredits", "location_phone", "location_website",
        "location_links", "location_description_fr", "location_access_fr",
        "attendancemode", "onlineaccesslink", "status", "age_min", "age_max", "country_fr", "links"
    ]
    # Garder uniquement les colonnes existantes parmi celles d√©sir√©es
    available_columns = [col for col in desired_columns if col in df.columns]
    df = df[available_columns].dropna(subset=["firstdate_begin"])

    # Conversion des dates en datetime
    df['firstdate_begin'] = pd.to_datetime(df['firstdate_begin'], errors='coerce')

    # Filtrer les √©v√©nements post√©rieurs au 1er janvier 2025
    df = df[(df['firstdate_begin'].notna()) & (df['firstdate_begin'] >= "2025-01-01")]
    if df.empty:
        print("‚ùå Aucun √©v√©nement trouv√© apr√®s le 1er janvier 2025.")
        exit()

    # Convertir la date en cha√Æne pour conserver le format ISO
    df['firstdate_begin'] = df['firstdate_begin'].dt.strftime("%Y-%m-%dT%H:%M:%S%z")

    df.to_json(RESPONSE_CLEAN_FILE, orient="records", indent=4, force_ascii=False)
    print(f"‚úÖ {len(df)} √©v√©nements nettoy√©s enregistr√©s dans {RESPONSE_CLEAN_FILE}")
    return df

def create_faiss_db(df):
    """
    G√©n√®re les embeddings et stocke les documents dans FAISS.
    """
    model = SentenceTransformer("sentence-transformers/all-MiniLM-L6-v2")
    if df.empty:
        print("‚ùå Aucun √©v√©nement √† indexer dans FAISS.")
        return

    texts = df.apply(lambda row: f"{row['title_fr']} - {row['description_fr']} ({row['conditions_fr']}, {row['firstdate_begin']})", axis=1).tolist()
    dates = df['firstdate_begin'].astype(str).tolist()

    print(f"‚úÖ {len(texts)} √©v√©nements pr√™ts pour l'indexation FAISS.")
    embeddings = model.encode(texts, convert_to_tensor=False)
    print(f"‚úÖ {len(embeddings)} embeddings g√©n√©r√©s avec succ√®s.")

    embedder = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
    documents = [Document(page_content=text, metadata={"date": date}) for text, date in zip(texts, dates)]
    print(f"‚úÖ {len(documents)} documents FAISS cr√©√©s avec m√©tadonn√©es.")

    vector_store = FAISS.from_documents(documents, embedder)
    vector_store.save_local(FAISS_INDEX_PATH)
    print(f"‚úÖ Base FAISS enregistr√©e avec {len(documents)} √©v√©nements.")

if __name__ == "__main__":
    print("üöÄ D√©but du processus de r√©cup√©ration et d'indexation des √©v√©nements...")
    fetch_data()  # R√©cup√©ration des donn√©es
    df = clean_and_filter_data()  # Nettoyage, filtrage et cr√©ation de RESPONSE_CLEAN_FILE
    create_faiss_db(df)  # Cr√©ation et sauvegarde de l'index FAISS

    # V√©rification des √©v√©nements index√©s dans FAISS
    print("\nüìå V√©rification des √©v√©nements index√©s dans FAISS :")
    retriever = FAISS.load_local(
        FAISS_INDEX_PATH,
        HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2"),
        allow_dangerous_deserialization=True
    ).as_retriever()

    print("üéâ Processus termin√© avec succ√®s !")
