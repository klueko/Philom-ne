import os
import json
import pandas as pd
import requests
from sentence_transformers import SentenceTransformer
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from langchain.schema import Document

# üìå 1. R√©cup√©ration des donn√©es depuis l'API Bordeaux M√©tropole
URL = "https://datahub.bordeaux-metropole.fr/api/explore/v2.1/catalog/datasets/met_agenda/records"
DATA_FILE = "reponse.json"
FAISS_INDEX_PATH = "faiss_index"

import time  # Ajout pour g√©rer les retries en cas d'√©chec

def fetch_data():
    """
    R√©cup√®re les donn√©es de Bordeaux M√©tropole et les sauvegarde en JSON.
    G√®re les erreurs r√©seau et les retries en cas d'√©chec.
    """
    all_data = []
    offset = 0
    limit = 100  # Nombre de r√©sultats par requ√™te
    max_records = 10000  # Nombre total de r√©sultats √† r√©cup√©rer
    max_retries = 5  # Nombre maximum de tentatives en cas d'√©chec

    while len(all_data) < max_records:
        params = {'limit': limit, 'offset': offset}
        retries = 0

        while retries < max_retries:
            try:
                response = requests.get(URL, params=params, timeout=10)
                
                if response.status_code == 200:
                    data = response.json().get('results', [])
                    if not data:
                        print("‚úÖ Fin des donn√©es (aucune nouvelle entr√©e trouv√©e).")
                        break  # Arr√™ter si aucune donn√©e retourn√©e
                    all_data.extend(data)
                    offset += limit
                    break  # Sortir de la boucle de retry apr√®s succ√®s

                else:
                    print(f"‚ö†Ô∏è Erreur HTTP {response.status_code}, tentative {retries + 1}/{max_retries}")
                    retries += 1
                    time.sleep(2)  # Attendre avant de r√©essayer

            except requests.exceptions.RequestException as e:
                print(f"‚ö†Ô∏è Erreur r√©seau : {e}, tentative {retries + 1}/{max_retries}")
                retries += 1
                time.sleep(2)  # Pause avant le retry

        if retries == max_retries:
            print(f"‚ùå √âchec apr√®s {max_retries} tentatives. Arr√™t de la r√©cup√©ration.")
            break  # Arr√™te compl√®tement si trop d'√©checs

    # Sauvegarde des donn√©es r√©cup√©r√©es
    with open(DATA_FILE, "w", encoding="utf-8") as file:
        json.dump(all_data, file, indent=4, ensure_ascii=False)

    print(f"‚úÖ {len(all_data)} √©v√©nements enregistr√©s dans {DATA_FILE}")


def clean_and_filter_data_bis():
    """
    Charge et filtre les donn√©es, en supprimant les colonnes inutiles et en filtrant les √©v√©nements apr√®s le 1er janvier 2025.
    """
    with open(DATA_FILE, "r", encoding="utf-8") as file:
        data = json.load(file)

    df = pd.json_normalize(data)

    # S√©lection des colonnes utiles
    columns_to_keep = ['title_fr', 'description_fr', 'conditions_fr', 'firstdate_begin']
    df = df[columns_to_keep].dropna()

    # ‚úÖ V√©rification des donn√©es brutes AVANT filtrage
    print("\nüìå V√©rification des √©v√©nements AVANT filtrage :")
    print(df[['title_fr', 'firstdate_begin']].head(20))  # Afficher les 20 premiers √©v√©nements

    # Convertir la date et filtrer apr√®s le 1er janvier 2025
    df['firstdate_begin'] = pd.to_datetime(df['firstdate_begin'], errors='coerce')

def clean_and_filter_data():
    """
    Charge et filtre les donn√©es, en supprimant les colonnes inutiles et en filtrant les √©v√©nements apr√®s le 1er janvier 2025.
    """
    with open(DATA_FILE, "r", encoding="utf-8") as file:
        data = json.load(file)

    df = pd.json_normalize(data)

    # S√©lection des colonnes utiles
    columns_to_keep = ['title_fr', 'description_fr', 'conditions_fr', 'firstdate_begin']
    df = df[columns_to_keep].dropna()

    # ‚úÖ Conversion des dates
    df['firstdate_begin'] = pd.to_datetime(df['firstdate_begin'], errors='coerce')

    print(df.head(10))

    # ‚úÖ V√©rification des dates avant filtrage
    invalid_dates = df[df['firstdate_begin'].isna()]
    if not invalid_dates.empty:
        print("\n‚ö†Ô∏è √âv√©nements avec une date NON VALIDE :")
        print(invalid_dates[['title_fr', 'firstdate_begin']])
        print(f"‚ùå {len(invalid_dates)} √©v√©nements ignor√©s √† cause d'une date invalide.")

    # ‚úÖ V√©rification des √©v√©nements supprim√©s apr√®s filtrage
    excluded_events = df[df['firstdate_begin'] <= "2025-01-01"]
    if not excluded_events.empty:
        print("\n‚ö†Ô∏è √âv√©nements EXCLUS apr√®s filtrage (dates ‚â§ 2025-01-01) :")
        print(excluded_events[['title_fr', 'firstdate_begin']])
        print(f"‚ùå {len(excluded_events)} √©v√©nements supprim√©s apr√®s filtrage.")

    # ‚úÖ Modification du filtrage pour √©viter de supprimer des √©v√©nements incorrectement
    df = df[(df['firstdate_begin'].notna()) & (df['firstdate_begin'] >= "2025-01-01")]

    if df.empty:
        print("‚ùå Aucun √©v√©nement trouv√© apr√®s le 1er janvier 2025.")
        exit()

    # ‚úÖ V√©rification des √©v√©nements restants par mois apr√®s filtrage
    df['mois'] = df['firstdate_begin'].dt.month
    df['ann√©e'] = df['firstdate_begin'].dt.year

    print("\nüìä Nombre d'√©v√©nements par mois apr√®s filtrage :")
    print(df.groupby(['ann√©e', 'mois']).size())

    # Supprime les colonnes temporaires apr√®s affichage
    df.drop(columns=['mois', 'ann√©e'], inplace=True)

    print(f"‚úÖ {len(df)} √©v√©nements conserv√©s apr√®s filtrage.")
    return df

def create_faiss_db(df):
    """
    G√©n√®re les embeddings et stocke les documents dans FAISS avec la date comme m√©tadonn√©e.
    """
    model = SentenceTransformer("sentence-transformers/all-MiniLM-L6-v2")

    # V√©rifier que df contient bien des √©v√©nements
    if df.empty:
        print("‚ùå Aucun √©v√©nement √† indexer dans FAISS.")
        return

    texts = df.apply(lambda row: f"{row['title_fr']} - {row['description_fr']} ({row['conditions_fr']}, {row['firstdate_begin']})", axis=1).tolist()
    dates = df['firstdate_begin'].astype(str).tolist()  # Convertir les dates en texte

    print(f"‚úÖ {len(texts)} √©v√©nements pr√™ts pour l'indexation FAISS.")

    # G√©n√©rer les embeddings
    embeddings = model.encode(texts, convert_to_tensor=False)
    print(f"‚úÖ {len(embeddings)} embeddings g√©n√©r√©s avec succ√®s.")

    # Cr√©ation des documents LangChain pour FAISS avec m√©tadonn√©es
    embedder = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
    documents = [Document(page_content=text, metadata={"date": date}) for text, date in zip(texts, dates)]

    # V√©rification avant indexation
    print(f"‚úÖ {len(documents)} documents FAISS cr√©√©s avec m√©tadonn√©es.")

    # Indexation dans FAISS
    vector_store = FAISS.from_documents(documents, embedder)
    vector_store.save_local(FAISS_INDEX_PATH)

    print(f"‚úÖ Base FAISS enregistr√©e avec {len(documents)} √©v√©nements.")

if __name__ == "__main__":
    print("üöÄ D√©but du processus de r√©cup√©ration et d'indexation des √©v√©nements...")
    
    fetch_data()  # √âtape 1 : R√©cup√©ration des donn√©es
    df = clean_and_filter_data()  # √âtape 2 : Nettoyage et filtrage
    create_faiss_db(df)  # √âtape 3 : Cr√©ation et sauvegarde de FAISS

    # üîç V√©rification des √©v√©nements index√©s dans FAISS
    print("\nüìå V√©rification des √©v√©nements index√©s dans FAISS :")
    retriever = FAISS.load_local(
        FAISS_INDEX_PATH,
        HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2"),
        allow_dangerous_deserialization=True
    ).as_retriever()  # ‚úÖ Important pour utiliser FAISS en mode recherche

    query_test = "mars 2025"
    docs = retriever.get_relevant_documents(query_test)  # ‚úÖ Correct

    if docs:
        print(f"‚úÖ {len(docs)} √©v√©nements trouv√©s pour '{query_test}' dans FAISS.")
        for doc in docs[:10]:  # üîç Affiche les 10 premiers r√©sultats
            print(doc.page_content)
    else:
        print(f"‚ùå Aucun √©v√©nement trouv√© pour la requ√™te : {query_test}")

    print("üéâ Processus termin√© avec succ√®s !")
